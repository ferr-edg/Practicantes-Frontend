# Sistema de Evaluaci贸n T茅cnica - Ingenier铆a de Software

Sistema para generar reportes de evaluaci贸n t茅cnica de practicantes en ingenier铆a de software.

##  Descripci贸n

Este sistema permite generar reportes detallados de evaluaci贸n t茅cnica que incluyen:
- Distribuci贸n de practicantes por nivel t茅cnico
- Listado de destacados para contrataci贸n
- Practicantes que requieren capacitaci贸n
- An谩lisis por competencia t茅cnica
- Detalle individual de cada practicante

##  Uso R谩pido

### 1. Preparar los datos de evaluaci贸n

Edita el archivo `datos_evaluacion_ejemplo.json` con la informaci贸n de tus practicantes o crea tu propio archivo JSON con el siguiente formato:

```json
{
  "fecha": "2024-01-15",
  "evaluador": "Nombre del Evaluador",
  "periodo": "Per铆odo evaluado",
  "observaciones": "Notas generales",
  "practicantes": [
    {
      "id": "P001",
      "nombre": "Nombre del Practicante",
      "puntaje_total": 20,
      "fortalezas": ["rea 1", "rea 2"],
      "areas_mejora": ["rea 3"],
      "competencias": {
        "Algoritmos y Estructuras de Datos": {"correctas": 10, "total": 10},
        "Bases de Datos": {"correctas": 8, "total": 10}
      }
    }
  ]
}
```

### 2. Generar el reporte

Ejecuta el script de Python:

```bash
python generador_evaluaciones.py datos_evaluacion_ejemplo.json reporte.md
```

O simplemente:

```bash
python generador_evaluaciones.py datos_evaluacion_ejemplo.json
```

El reporte se generar谩 autom谩ticamente en formato Markdown.

##  Niveles de Evaluaci贸n

El sistema clasifica a los practicantes en los siguientes niveles:

- **Excelente (20pts):** Puntaje perfecto
- **Competente (17-15pts):** Buen nivel t茅cnico
- **B谩sico (14-10pts):** Nivel aceptable con 谩reas de mejora
- **Deficiente (09-05pts):** Requiere capacitaci贸n significativa
- **Sin conocimiento (00pts):** Sin conocimientos t茅cnicos demostrados

##  Estructura de Archivos

```
evaluacion_tecnica/
 generador_evaluaciones.py      # Script principal para generar reportes
 template_evaluacion.md         # Plantilla manual de evaluaci贸n
 datos_evaluacion_ejemplo.json   # Ejemplo de datos de entrada
 README.md                       # Este archivo
```

##  Requisitos

- Python 3.6 o superior
- No se requieren librer铆as externas (usa solo la biblioteca est谩ndar)

##  Formato de Datos

### Estructura del JSON de entrada:

- **fecha:** Fecha de la evaluaci贸n (formato: YYYY-MM-DD)
- **evaluador:** Nombre del evaluador
- **periodo:** Per铆odo evaluado
- **observaciones:** Notas generales sobre la evaluaci贸n
- **practicantes:** Array de objetos con:
  - **id:** Identificador 煤nico del practicante
  - **nombre:** Nombre completo
  - **puntaje_total:** Puntaje total (0-20)
  - **fortalezas:** Array de 谩reas donde destaca
  - **areas_mejora:** Array de 谩reas que necesita mejorar
  - **competencias:** Objeto con estad铆sticas por competencia

##  Competencias T茅cnicas Evaluadas

El sistema eval煤a las siguientes competencias:

1. Algoritmos y Estructuras de Datos
2. Bases de Datos
3. Arquitectura de Software
4. Testing y QA
5. DevOps y CI/CD
6. Seguridad
7. APIs y Microservicios
8. Patrones de Dise帽o
9. Versionado (Git)
10. Frameworks y Librer铆as

##  Ejemplo de Uso

```python
from generador_evaluaciones import GeneradorEvaluaciones
import json

# Cargar datos
with open('datos_evaluacion.json', 'r') as f:
    datos = json.load(f)

# Generar reporte
generador = GeneradorEvaluaciones(datos)
generador.guardar_reporte('mi_reporte.md')
```

##  Personalizaci贸n

Puedes modificar el archivo `generador_evaluaciones.py` para:
- Agregar nuevas competencias t茅cnicas
- Cambiar los umbrales de puntuaci贸n
- Modificar el formato del reporte
- Agregar nuevas m茅tricas o an谩lisis

##  Soporte

Para dudas o mejoras, revisa el c贸digo fuente o modifica seg煤n tus necesidades.

---

**Versi贸n:** 1.0  
**ltima actualizaci贸n:** 2024

